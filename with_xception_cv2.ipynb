{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.applications import Xception, xception\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Dense, Input, Lambda\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:00<00:00, 413.37it/s]\n",
      " 12%|█▏        | 895/7393 [00:04<00:29, 222.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Abyssinian_102.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1036/7393 [00:04<00:28, 222.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Abyssinian_100.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1438/7393 [00:06<00:26, 223.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Egyptian_Mau_139.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2750/7393 [00:12<00:20, 222.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Egyptian_Mau_145.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4227/7393 [00:18<00:13, 226.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Egyptian_Mau_177.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 5493/7393 [00:24<00:08, 227.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Abyssinian_101.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 5637/7393 [00:24<00:07, 226.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Egyptian_Mau_191.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 5783/7393 [00:25<00:07, 226.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Abyssinian_34.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 7274/7393 [00:32<00:00, 227.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read extra train image failed: ../dogs-vs-cats-dataset/images-Oxford-IIIT/Egyptian_Mau_167.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7393/7393 [00:32<00:00, 227.15it/s]\n",
      "100%|██████████| 12500/12500 [00:30<00:00, 414.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 32384\n",
      "Label size: 32384\n",
      "Testing data size: 12500\n"
     ]
    }
   ],
   "source": [
    "data_path_train = '../dogs-vs-cats-dataset/train'\n",
    "data_path_train_extra = '../dogs-vs-cats-dataset/images-Oxford-IIIT'\n",
    "data_path_test = '../dogs-vs-cats-dataset/test'\n",
    "image_names_train = os.listdir(data_path_train)\n",
    "image_names_train_extra = os.listdir(data_path_train_extra)\n",
    "image_names_test = os.listdir(data_path_test)\n",
    "input_shape = (299, 299, 3)\n",
    "labels = []\n",
    "trains = []\n",
    "tests = []\n",
    "\n",
    "# 处理标准的训练数据\n",
    "for i in tqdm(range(len(image_names_train))):\n",
    "    image_name = image_names_train[i]\n",
    "    image_path = os.path.join(data_path_train, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print('Read train image failed:', image_path)\n",
    "        continue\n",
    "    image = cv2.resize(image, (input_shape[0], input_shape[1]))\n",
    "    trains.append(image[:, :, ::-1])\n",
    "    # cat: 0, dog: 1\n",
    "    category = 1 if 'dog' in image_name else 0\n",
    "    labels.append(category)\n",
    "\n",
    "    \n",
    "# 猫的种类\n",
    "cat_types = ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian',\n",
    "             'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx']\n",
    "# 处理扩展的训练数据\n",
    "for i in tqdm(range(len(image_names_train_extra))):\n",
    "    image_name = image_names_train_extra[i]\n",
    "    image_path = os.path.join(data_path_train_extra, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print('Read extra train image failed:', image_path)\n",
    "        continue\n",
    "    image = cv2.resize(image, (input_shape[0], input_shape[1]))\n",
    "    index = len(image_names_train) + i\n",
    "    trains.append(image[:, :, ::-1])\n",
    "    \n",
    "    # 获取动物的种类（dog or cat）\n",
    "    spt = image_names_train_extra[i].split('_')\n",
    "    spt.pop()\n",
    "    tp = '_'.join(spt)\n",
    "    category = 0 if tp in cat_types else 1\n",
    "    labels.append(category)\n",
    "    \n",
    "\n",
    "# 处理标准的测试数据\n",
    "for i in tqdm(range(len(image_names_test))):\n",
    "    image_name = image_names_test[i]\n",
    "    image_path = os.path.join(data_path_test, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print('Read test image failed:', image_path)\n",
    "        continue\n",
    "    image = cv2.resize(image, (input_shape[0], input_shape[1]))\n",
    "    tests.append(image[:, :, ::-1])\n",
    "    \n",
    "    \n",
    "trains = np.array(trains)\n",
    "labels = np.array(labels)\n",
    "tests = np.array(tests)\n",
    "\n",
    "print('Training data size: %d' % len(trains))\n",
    "print('Label size: %d' % len(labels))\n",
    "print('Testing data size: %d' % len(tests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=input_shape)\n",
    "x = Lambda(xception.preprocess_input)(x)\n",
    "model = Xception(input_tensor=x, input_shape=input_shape, weights='imagenet', include_top=False, pooling='avg')\n",
    "bottleneck_features_train = model.predict(trains, batch_size=128)\n",
    "bottleneck_features_test = model.predict(tests, batch_size=128)\n",
    "\n",
    "with h5py.File(\"bottleneck_features_with_extra_data.h5\", 'w') as h:\n",
    "    h.create_dataset('trains', data=bottleneck_features_train)\n",
    "    h.create_dataset('labels', data=labels)\n",
    "    h.create_dataset('tests', data=bottleneck_features_test)\n",
    "    h.create_dataset('test_imgs', data=image_names_test)\n",
    "\n",
    "print('bottleneck features have been wrote to bottleneck_features_with_extra_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready!\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('bottleneck_features.h5','r') as h:\n",
    "# with h5py.File('bottleneck_features_with_extra_data.h5','r') as h:\n",
    "    X_train = np.array(h['trains'])\n",
    "    y_train = np.array(h['labels'])\n",
    "    X_test = np.array(h['tests'])\n",
    "    test_imgs = np.array(h['test_imgs'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=True, test_size=0.2, random_state=2018)\n",
    "\n",
    "x = Input(shape=(X_train.shape[1],))\n",
    "y = Dropout(0.2)(x)\n",
    "y = Dense(1, activation='sigmoid')(y)\n",
    "model = Model(x, y)\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Model ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(mod, file_name):\n",
    "    y_pred = mod.predict(X_test, verbose=1)\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "\n",
    "    df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "    for i in range(len(test_imgs)):\n",
    "        image_name = test_imgs[i]\n",
    "        index = int(str.split(image_name, '.')[0]) - 1\n",
    "        df.iat[index, 1] = y_pred[i]\n",
    "\n",
    "    df.to_csv(os.path.join('./predict-csv', file_name), index=None)\n",
    "    print('The prediction result has been wrote to: ', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回调函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        predict_func(self.model, 'predict' + '_epoch' + str(epoch + 1) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练与优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 0.1346 - acc: 0.9815 - val_loss: 0.0476 - val_acc: 0.9906\n",
      "12500/12500 [==============================] - 0s 40us/step\n",
      "The prediction result has been wrote to:  predict_epoch1.csv\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.0334 - acc: 0.9930 - val_loss: 0.0297 - val_acc: 0.9916\n",
      "12500/12500 [==============================] - 0s 30us/step\n",
      "The prediction result has been wrote to:  predict_epoch2.csv\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.0231 - acc: 0.9937 - val_loss: 0.0259 - val_acc: 0.9924\n",
      "12500/12500 [==============================] - 0s 30us/step\n",
      "The prediction result has been wrote to:  predict_epoch3.csv\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0202 - acc: 0.9944 - val_loss: 0.0247 - val_acc: 0.9922\n",
      "12500/12500 [==============================] - 0s 30us/step\n",
      "The prediction result has been wrote to:  predict_epoch4.csv\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0241 - val_acc: 0.9922\n",
      "12500/12500 [==============================] - 0s 29us/step\n",
      "The prediction result has been wrote to:  predict_epoch5.csv\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.0168 - acc: 0.9950 - val_loss: 0.0236 - val_acc: 0.9920\n",
      "12500/12500 [==============================] - 0s 30us/step\n",
      "The prediction result has been wrote to:  predict_epoch6.csv\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0158 - acc: 0.9954 - val_loss: 0.0231 - val_acc: 0.9924\n",
      "12500/12500 [==============================] - 0s 30us/step\n",
      "The prediction result has been wrote to:  predict_epoch7.csv\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0156 - acc: 0.9950 - val_loss: 0.0229 - val_acc: 0.9924\n",
      "12500/12500 [==============================] - 0s 30us/step\n",
      "The prediction result has been wrote to:  predict_epoch8.csv\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.0143 - acc: 0.9956 - val_loss: 0.0231 - val_acc: 0.9922\n",
      "12500/12500 [==============================] - 0s 30us/step\n",
      "The prediction result has been wrote to:  predict_epoch9.csv\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0230 - val_acc: 0.9924\n",
      "12500/12500 [==============================] - 0s 30us/step\n",
      "The prediction result has been wrote to:  predict_epoch10.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6d591ca90>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[LossCallback()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 1s 82us/step\n",
      "The prediction result has been wrote to predict.csv\n"
     ]
    }
   ],
   "source": [
    "predict_func(model, 'predict.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
